{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5726a861",
   "metadata": {},
   "source": [
    "# 1. Age Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01c083e",
   "metadata": {},
   "source": [
    "### 1.6 Core Adjustments of the CNN baseline models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb98d35",
   "metadata": {},
   "source": [
    "(a) Import libraries\n",
    "\n",
    "(b) Load the saved training/validation/testing datasets\n",
    "\n",
    "(c) Check GPU availability with TensorFlow\n",
    "\n",
    "(d) Decision on what hyperparameters to tune for the baeline CNN models\n",
    "\n",
    "(e) Define reusable CNN building functions for Regression amd Classification\n",
    "\n",
    "(f) Define a function to free up memory for GPU after model training\n",
    "\n",
    "(g) Define 2 functions to create 2 sfuffled datasets for tuning\n",
    "\n",
    "(h) Define 2 functions to create 2 sfuffle datasets for final training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edd4f68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (a) Import libraries\n",
    "# --------------------------------------------------------------------\n",
    "import tensorflow as tf                             # includes Keras and all its functionalities. It is used for building and training neural networks models such as CNNs.\n",
    "\n",
    "# Enable GPU memory growth before importing other libraries\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)     # Prevent TensorFlow from allocating all GPU memory at once, avoiding out-of-memory errors (= crashes during training)\n",
    "\n",
    "# Clear previous sessions to free up resources (before starting new model training) to avoid potential out-of-memory errors (= crashes during training)\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "import gc                           # for garbage collection to free up memory\n",
    "\n",
    "import matplotlib.pyplot as plt     # for creating visualizations in Python.\n",
    "import pandas as pd                 # for data manipulation and analysis, particularly for working with DataFrames.  \n",
    "import numpy as np                  # for numerical computing (e.g. arrays and  mathematical functions).\n",
    "import os, pathlib                  # for interacting with the operating system and handling file paths.\n",
    "\n",
    "\n",
    "# from tensorflow import keras                      # not needed since tf includes keras\n",
    "# from tensorflow.keras import layers, models       # not needed since tf includes keras\n",
    "\n",
    "# --------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "190e8b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (b) Load the saved training/validation/testing datasets\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "# training/validation/testing datasets for normalised images (Feature inputs: pixel values)\n",
    "X_train_norm = np.load('X_train_norm.npy')              # scaled images between 0 and 1 \n",
    "X_val_norm = np.load('X_val_norm.npy')\n",
    "X_test_norm = np.load('X_test_norm.npy')\n",
    "\n",
    "# training/validation/testing datasets for regression (Target features labels: age in years)\n",
    "y_train_reg = np.load('y_train_reg.npy')                # converted from default int64 to float32\n",
    "y_val_reg = np.load('y_val_reg.npy')\n",
    "y_test_reg = np.load('y_test_reg.npy')\n",
    "\n",
    "# training/validation/testing datasets for classification (Target features labels: age groups)\n",
    "y_train_cls_ohe = np.load('y_train_cls_ohe.npy')        # transformed from integer labels to one-hot encoded format (2D arrays with binary values for each class)\n",
    "y_val_cls_ohe = np.load('y_val_cls_ohe.npy')\n",
    "y_test_cls_ohe = np.load('y_test_cls_ohe.npy')\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "# to reduce memory usage, convert X data splits to float16\n",
    "X_train_norm = X_train_norm.astype('float16')\n",
    "X_val_norm   = X_val_norm.astype('float16')\n",
    "X_test_norm  = X_test_norm.astype('float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f71cc7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.0\n",
      "GPUs available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Built with CUDA: True\n"
     ]
    }
   ],
   "source": [
    "# (c) Check GPU availability with TensorFlow\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "# Print TF version\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# List available GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"GPUs available:\", gpus)\n",
    "\n",
    "# Check if built with CUDA\n",
    "print(\"Built with CUDA:\", tf.test.is_built_with_cuda())\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# Check GPU availability with PyTorch\n",
    "#import torch\n",
    "#print(torch.cuda.is_available())\n",
    "#print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU detected\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# NOTE on GPU usage with TensorFlow in this environment:\n",
    "\n",
    "# TensorFlow cannot use the GPU in the current environment apparently for 2 reasons:\n",
    "# 1. Python 3.11 is installed, but the last Windows GPU-enabled TensorFlow (2.10) only supports Python 3.7–3.9. So TF 2.10 cannot be installed.\n",
    "# 2. TensorFlow 2.11+ removed all GPU support for Windows entirely. # Result: Any TensorFlow version installed in this environment is CPU-only.\n",
    "\n",
    "# ---> As TensorFlow will be use for this assignment,the solution is to create a new conda environment with Python 3.9 and install TensorFlow 2.10 with GPU support\n",
    "\n",
    "# On Anaconda Prompt:\n",
    "# conda create -n adl_tf_gpu_env python=3.9 -y\n",
    "# conda activate adl_tf_gpu_env\n",
    "# pip install tensorflow==2.10\n",
    "# pip install matplotlib pandas scikit-learn\n",
    "\n",
    "# Separately:\n",
    "# download and install CUDA Toolkit 11.2 and cuDNN 8.1.0 for CUDA 11.2 from NVIDIA website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecb7dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (d) Decision on what hyperparameters to tune with TensorFlow for the baeline CNN models both for regression and classification\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "# For the baseline model, we should only tune the hyperparameters that influence training stability and general model behaviour, \n",
    "# avoiding tuning anything that would change the model architecture or its capacity to learn complex patterns.\n",
    "\n",
    "# Therefore, we will tune the following basic hyperparameters:\n",
    "\n",
    "# 1. Learning Rate: This controls how much to change the model in response to the estimated error each time the model weights are updated.\n",
    "# 2. Batch Size: This defines the number of samples that will be propagated through the network at once.\n",
    "# 3. Number of Epochs: This defines the number of complete passes through the training dataset.\n",
    "# 4. Optimizer type: Different optimizers can have different effects on training dynamics.\n",
    "# 5. Dropout Rate: If dropout layers are used, tuning the dropout rate can help prevent overfitting.\n",
    "# 6. Early Stopping Patience: This defines how many epochs to wait for an improvement in validation loss before stopping training early.\n",
    "\n",
    "# For the baseline model, we will NOT tune:\n",
    "\n",
    "# CNN Kernel Variations: \n",
    "    # 1. Activation Functions: We will keep the default ReLU for hidden layers and linear/softmax for output layers (as per regression/classification).\n",
    "    # 2. Kernel size: We will keep the default kernel sizes for convolutional layers (3x3).\n",
    "\n",
    "# Network Architecture Variations:\n",
    "    # 3. Network Depth and Width: We will keep the default number of layers and number of filters/neurons per layer (32 and 64 filters (unique type of image features detectable) in conv layers; 128 neurons in dense layers).\n",
    "    # 4. Pooling presence Vs absence: We will keep the default max pooling layers after convolutional layers (2x2).\n",
    "    # 5. Skip connections presence Vs absence: We will not add skip connections in the baseline model.\n",
    "    # 6. Fully connected layers variations: We will keep the default of 2 dense layers after flattening the conv layers output (hidden dense layer with 128 neurons and the output layer).\n",
    "\n",
    "# Dataset Variations:\n",
    "    # 7. 1 vs 3 channel input images: We will use 3-channel RGB images for the baseline model.\n",
    "    # 8. Data Augmentation: We will not apply data augmentation techniques for the baseline model.\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Hyperparameter ranges for tuning\n",
    "# --------------------------------------------------------------------\n",
    "# Learning rate: Common starting point for Adam is 1e-3(0.001); for SGD usually 1e-2(0.01) to 1e-4(0.0001)\n",
    "\n",
    "# Batch size: 32-128 is common; larger batches require more GPU memory\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam\n",
    "# https://www.tensorflow.org/tutorials/keras/overfit_and_underfit\n",
    "\n",
    "# Epochs: 50-200 is typical; early stopping is used to prevent overfitting.\n",
    "\n",
    "# Dropout rate: 0.3-0.5 is common for dense layers\n",
    "# https://www.tensorflow.org/tutorials/keras/overfit_and_underfit\n",
    "\n",
    "# EarlyStopping patience: 3-10 epochs is standard for validation monitoring\n",
    "\n",
    "# Optimizer: 'adam' (adaptive) vs 'sgd' (classic) are standard choices for CNNs\n",
    "\n",
    "learning_rates = [1e-4, 1e-3, 1e-2]     # Tunable learning rates\n",
    "batch_sizes = [32, 64, 128]             # Tunable batch sizes\n",
    "dropout_rates = [0.3, 0.5]              # Dropout rates for regularization\n",
    "optimizers = ['adam']                   # It is decided to only use 'adam' optimizer for tuning to reduce computational load (also sgd requires more careful learning rate tuning)\n",
    "early_stopping_patiences = [3, 5, 8]    # Patience for EarlyStopping\n",
    "num_epochs_options = [50, 80, 120]      # Max number of epochs to train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04274616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (e) Define 2 functions to build the baseline CNN model for regression and classification\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "# In CNNs, convolutional layers with ReLU activation are typically followed by pooling layers to reduce spatial dimensions and extract dominant features (such as edges, textures, shapes) from images.\n",
    "# After 2 conv+pooling layers, the 2D feature maps are flattened into 1D feature vectors to be processed by fully connected (dense) layers.\n",
    "# Dropout layers are usually added before the output layer to prevent overfitting by randomly dropping a fraction of neurons during training.\n",
    "\n",
    "    # Convolutional layers use small learnable filters (kernels), 3×3 matrices, which slide over the input image to detect visual features.\n",
    "        # Each filter has its own set of weights and learns to detect a specific pattern (edges, textures, shapes, etc.).\n",
    "\n",
    "        # ReLU (Rectified Linear Unit) activation is used to introduce non-linearity, allowing the model to learn non-linear patterns (curves, edges, textures, shape)).\n",
    "    \n",
    "    # Pooling layers (2x2 matrices of max values of previously learned weights)reduce the spatial dimensions of feature maps, retaining the most important features \n",
    "        # while reducing computational load and helping to prevent overfitting. \n",
    "\n",
    "    # Flatten layers convert 2D feature maps (the output of the last convolutional/pooling layer) into 1D vectors for input into dense layers.\n",
    "\n",
    "    # Dense layers learn high-level representations and relationships between features (such as combinations of edges, textures, shapes) extracted by convolutional layers. \n",
    "        # Each layer is fully connected to the previous layer, allowing complex combinations of features to be learned.\n",
    "        # These layers operate on abstracted features rather than raw pixels.\n",
    "\n",
    "    # Dropout layers randomly deactivate a fraction (specified by the dropout rate) of neurons (the less important ones based on their weights) during training.\n",
    "\n",
    "    # The output layer, for regression tasks, has a single neuron with linear activation to predict continuous values (between 0 and 110 for age).\n",
    "    # The output layer, for classification tasks, has neurons equal to the number of classes with softmax activation to output class probabilities (between 0 and 1 for each class).\n",
    "\n",
    "def build_base_cnn_reg(input_shape=(128, 128, 3), dropout_rate=None):\n",
    "    \"\"\"\n",
    "    Builds a base CNN model for regression (predicting age).\n",
    "    \n",
    "    Layers:\n",
    "    - Conv2D + MaxPooling2D for feature extraction of images\n",
    "    - Flatten the 2D feature maps into 1D feature vectors\n",
    "    - Dense 128 neurons + Dropout (before the output layer) for regularization (adding constraints to a model to reduce overfitting and improve its ability to generalize to new, unseen data)\n",
    "    - Dense 1 neuron for regression output\n",
    "    \n",
    "    Args:\n",
    "        input_shape (tuple): Shape of input images (height, width, channels)\n",
    "        dropout_rate (float): Dropout rate to prevent overfitting\n",
    "        \n",
    "    Returns:\n",
    "        tf.keras.Sequential model\n",
    "    \"\"\"\n",
    "    layers = [\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "    ]\n",
    "\n",
    "    if dropout_rate is not None:\n",
    "        layers.append(tf.keras.layers.Dropout(dropout_rate))        # Insert dropout before the output layer\n",
    "    \n",
    "    layers.append(tf.keras.layers.Dense(1))  # Regression output\n",
    "    \n",
    "    return tf.keras.Sequential(layers)\n",
    "\n",
    "\n",
    "def build_base_cnn_cls(input_shape=(128, 128, 3), dropout_rate=None, num_classes=y_test_cls_ohe.shape[1]):  # number of classes inferred from one-hot encoded labels\n",
    "    \"\"\"\n",
    "    Builds a base CNN model for classification (predicting age category).\n",
    "    \n",
    "    Layers:\n",
    "    - Conv2D + MaxPooling2D for feature extraction\n",
    "    - Flatten\n",
    "    - Dense 128 neurons + Dropout for regularization\n",
    "    - Dense num_classes neurons with softmax activation\n",
    "    \n",
    "    Args:\n",
    "        input_shape (tuple): Shape of input images (height, width, channels)\n",
    "        dropout_rate (float): Dropout rate to prevent overfitting\n",
    "        num_classes (int): Number of output classes\n",
    "        \n",
    "    Returns:\n",
    "        tf.keras.Sequential model\n",
    "    \"\"\"\n",
    "    layers = [\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation='relu')\n",
    "    ]\n",
    "\n",
    "    if dropout_rate is not None:\n",
    "        layers.append(tf.keras.layers.Dropout(dropout_rate))            # Insert dropout before the output layer\n",
    "\n",
    "    layers.append(tf.keras.layers.Dense(num_classes, activation='softmax'))  # Classification output\n",
    "\n",
    "    return tf.keras.Sequential(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "055b1a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (f) Define a function to free up memory for GPU after model training\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "# To avoid out-of-memory errors during hyperparameter tuning with multiple model trainings,\n",
    "# we define a function to clear GPU memory after each model training.\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"Function to clear GPU memory after model training to avoid out-of-memory errors.\"\"\"\n",
    "    tf.keras.backend.clear_session()            # Clear Keras session\n",
    "    gc.collect()                                # Run garbage collection (such as unused variables, tensors, etc., that are no longer needed and use GPU memory) \n",
    "# --------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7b508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (g) Define 2 functions to create 2 sfuffled datasets for tuning (regression and classification)\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "# training dataset is shuffled to improve generalization by exposing the model to different data orders each epoch; \n",
    "# it prevents the model from learning patterns in the order of the data.\n",
    "\n",
    "# validation dataset is not shuffled as we want the same inputs in the same order every time during valuation; unlike the training set, the model does not update weights from validation data, \n",
    "# so shuffling is unnecessary\n",
    "\n",
    "# .prefetch tells TensorFlow to prepare the next n batches in the background while the model is training on the current batch;\n",
    "# prefetch(1) is usually enough for small dataset;\n",
    "# prefetching more batches consumes more memory: keeping prefetch(1) is safest to avoid out-of-memory error using GPU.\n",
    "\n",
    "def create_datasets_tuning_reg (X_train_norm, y_train_reg, X_val_norm, y_val_reg, batch_size):\n",
    "    train_dataset_reg = tf.data.Dataset.from_tensor_slices((X_train_norm, y_train_reg))\n",
    "    train_dataset_reg = train_dataset_reg.shuffle(buffer_size=1000).batch(batch_size).prefetch(1)     # shuffle training dataset to improve generalization  \n",
    "    \n",
    "    val_dataset_reg = tf.data.Dataset.from_tensor_slices((X_val_norm, y_val_reg))\n",
    "    val_dataset_reg = val_dataset_reg.batch(batch_size)                                             # validation dataset is not shuffled as randomness is not desired during evaluation\n",
    "    \n",
    "    return train_dataset_reg, val_dataset_reg\n",
    "\n",
    "def create_datasets_tuning_cls (X_train_norm, y_train_cls_ohe, X_val_norm, y_val_cls_ohe, batch_size):\n",
    "    train_dataset_cls = tf.data.Dataset.from_tensor_slices((X_train_norm, y_train_cls_ohe))\n",
    "    train_dataset_cls = train_dataset_cls.shuffle(buffer_size=1000).batch(batch_size).prefetch(1)\n",
    "    \n",
    "    val_dataset_cls = tf.data.Dataset.from_tensor_slices((X_val_norm, y_val_cls_ohe))\n",
    "    val_dataset_cls = val_dataset_cls.batch(batch_size)                                                        \n",
    "    \n",
    "    return train_dataset_cls, val_dataset_cls\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# (h) Define 2 functions to create 2 sfuffle datasets for final training and evaluation (regression and classification)\n",
    "# --------------------------------------------------------------------\n",
    "def create_datasets_final_reg (X_train_norm, y_train_reg, X_val_norm, y_val_reg, X_test_norm, y_test_reg, batch_size):\n",
    "    train_dataset_reg = tf.data.Dataset.from_tensor_slices((X_train_norm, y_train_reg))\n",
    "    train_dataset_reg = train_dataset_reg.shuffle(buffer_size=1000).batch(batch_size).prefetch(1)\n",
    "\n",
    "    val_dataset_reg = tf.data.Dataset.from_tensor_slices((X_val_norm, y_val_reg))\n",
    "    val_dataset_reg = val_dataset_reg.batch(batch_size)\n",
    "    \n",
    "    test_dataset_reg = tf.data.Dataset.from_tensor_slices((X_test_norm, y_test_reg))\n",
    "    test_dataset_reg = test_dataset_reg.batch(batch_size)\n",
    "    \n",
    "    return train_dataset_reg, val_dataset_reg, test_dataset_reg\n",
    "\n",
    "def create_datasets_final_cls (X_train_norm, y_train_cls_ohe, X_val_norm, y_val_cls_ohe, X_test_norm, y_test_cls_ohe, batch_size):\n",
    "    train_dataset_cls = tf.data.Dataset.from_tensor_slices((X_train_norm, y_train_cls_ohe))\n",
    "    train_dataset_cls = train_dataset_cls.shuffle(buffer_size=1000).batch(batch_size).prefetch(1)\n",
    "\n",
    "    val_dataset_cls = tf.data.Dataset.from_tensor_slices((X_val_norm, y_val_cls_ohe))\n",
    "    val_dataset_cls = val_dataset_cls.batch(batch_size)\n",
    "    \n",
    "    test_dataset_cls = tf.data.Dataset.from_tensor_slices((X_test_norm, y_test_cls_ohe))\n",
    "    test_dataset_cls = test_dataset_cls.batch(batch_size)\n",
    "    \n",
    "    return train_dataset_cls, val_dataset_cls, test_dataset_cls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9620f0c3",
   "metadata": {},
   "source": [
    "#### 1.6.1 Tune Basic Hyperparameters: learning_rates, batch_sizes\n",
    "Batch + LR change how the model learns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c98cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune Basic Hyperparameters with TensorFlow: learning rate, batch size, number of epochs = 20\n",
    "# Combined hyperparameter tuning for regression and classification models instead of separate tuning loops to cut the time of execution by half.\n",
    "\n",
    "# Define hyperparameters to tune\n",
    "learning_rates = [0.001, 0.0005]            # 0.001 means the model weights are updated by 0.1% of the calculated gradient at each step\n",
    "batch_sizes = [32, 64]                      # Number of samples (independent units of data (images) being processed together) per gradient update (in each training iteration)\n",
    "num_epochs = 20                             # Number of complete passes through the training dataset to update model weights\n",
    "\n",
    "# These hyperparameter ranges are chosen based on common practices for CNN training and to balance computational efficiency with effective model training.\n",
    "\n",
    "# Track best results\n",
    "\n",
    "# --- REGRESSION MODEL ---\n",
    "best_val_loss_reg = float('inf')            # Initialize best validation loss to infinity(for minimization, as lower is better)\n",
    "best_hyperparams_reg = {}                   # Dictionary to store best hyperparameters for regression\n",
    "\n",
    "# --- CLASSIFICATION MODEL ---\n",
    "best_val_loss_cls = float('inf')            # Initialize best validation loss to infinity(for minimization, as lower is better)\n",
    "best_hyperparams_cls = {}                   # Dictionary to store best hyperparameters for classification\n",
    "\n",
    "# Combined Tuning Loop for REGRESSION MODEL\n",
    "\n",
    "for lr_reg in learning_rates:\n",
    "        for batch_size in batch_sizes:\n",
    "            print(f'Tuning with Learning Rate (Reg): {lr_reg}, Batch Size: {batch_size}')\n",
    "                        \n",
    "            # Clear previous models and free up memory to avoid potential out-of-memory errors during training\n",
    "            clear_gpu_memory()\n",
    "\n",
    "            # Create tf.data datasets (training + validation) using the function to avoid OOM (out-of-memory) errors\n",
    "            train_dataset_reg, val_dataset_reg = create_datasets_tuning_reg(X_train_norm, y_train_reg, X_val_norm, y_val_reg, batch_size)\n",
    "\n",
    "\n",
    "            # Build (using the function) and compile regression model\n",
    "            model_reg = build_base_cnn_reg(dropout_rate=None)           # Build regression model without dropout as it will be tuned later\n",
    "\n",
    "            model_reg.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_reg),\n",
    "                                loss='mean_squared_error',\n",
    "                                metrics=['mean_absolute_error'])                     \n",
    "            \n",
    "            # Train regression model\n",
    "            history_reg = model_reg.fit(train_dataset_reg,                      # history object stores training history (loss and metrics values at each epoch)\n",
    "                                        validation_data=val_dataset_reg,\n",
    "                                        epochs=num_epochs,\n",
    "                                        verbose=2)\n",
    "            \n",
    "            # Evaluate regression model on validation set\n",
    "            val_loss_reg = min(history_reg.history['val_loss'])                  # Get the minimum validation loss from the list of validation losses for all epochs of that run\n",
    "            print(f'Regression Validation Loss: {val_loss_reg}')\n",
    "            \n",
    "            # Update best regression hyperparameters if improved at end of every run (tuning iteration)\n",
    "            if val_loss_reg < best_val_loss_reg:\n",
    "                best_val_loss_reg = val_loss_reg\n",
    "                best_hyperparams_reg = {'learning_rate': lr_reg, 'batch_size': batch_size}\n",
    "            \n",
    "            # Clear previous models and free up memory\n",
    "            clear_gpu_memory()\n",
    "\n",
    "# Combined Tuning Loop for CLASSIFICATION MODEL\n",
    "            \n",
    "for lr_cls in learning_rates:\n",
    "        for batch_size in batch_sizes:\n",
    "            print (f'Tuning with Learning Rate (Cls): {lr_cls}, Batch Size: {batch_size}')\n",
    "                        \n",
    "            # Clear previous models and free up memory            \n",
    "            clear_gpu_memory()\n",
    "\n",
    "            # Create tf.data datasets (training + validation) using the function to avoid OOM (out-of-memory) errors\n",
    "            train_dataset_cls, val_dataset_cls = create_datasets_tuning_cls(X_train_norm, y_train_cls_ohe, X_val_norm, y_val_cls_ohe, batch_size)\n",
    "                        \n",
    "            # Build (using the function) and compile classification model\n",
    "            model_cls = build_base_cnn_cls(dropout_rate=None)          # Build classification model without dropout as it will be tuned later\n",
    "            \n",
    "            model_cls.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_cls),\n",
    "                                loss='categorical_crossentropy',\n",
    "                                metrics=['accuracy'])\n",
    "            \n",
    "            # Train classification model\n",
    "            history_cls = model_cls.fit(train_dataset_cls,\n",
    "                                        validation_data=val_dataset_cls,\n",
    "                                        epochs=num_epochs,\n",
    "                                        verbose=2)\n",
    "            \n",
    "            # Evaluate classification model on validation set\n",
    "            val_loss_cls = min(history_cls.history['val_loss'])\n",
    "            print(f'Classification Validation Loss: {val_loss_cls}')\n",
    "            \n",
    "            # Update best classification hyperparameters if improved\n",
    "            if val_loss_cls < best_val_loss_cls:\n",
    "                best_val_loss_cls = val_loss_cls\n",
    "                best_hyperparams_cls = {'learning_rate': lr_cls, 'batch_size': batch_size}\n",
    "            \n",
    "            # Clear models and free GPU memory after each full iteration\n",
    "            clear_gpu_memory()\n",
    "\n",
    "# Display best hyperparameters found\n",
    "print(\"Best Hyperparameters for Regression:\")\n",
    "print(best_hyperparams_reg)\n",
    "print(f\"Best Validation Loss for Regression: {best_val_loss_reg}\")\n",
    "\n",
    "print(\"\\nBest Hyperparameters for Classification:\")\n",
    "print(best_hyperparams_cls)\n",
    "print(f\"Best Validation Loss for Classification: {best_val_loss_cls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c05fd8c",
   "metadata": {},
   "source": [
    "Tuning Basic Parameters Result:\n",
    "\n",
    "Best Hyperparameters for Regression: {'learning_rate': 0.0005, 'batch_size': 32}\n",
    "\n",
    "Best Validation Loss for Regression: 614.8016967773438\n",
    "\n",
    "Best Hyperparameters for Classification: {'learning_rate': 0.0005, 'batch_size': 32}\n",
    "\n",
    "Best Validation Loss for Classification: 1.349984884262085\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef4f8a9",
   "metadata": {},
   "source": [
    "#### 1.6.2 Tune Basic Hyperparameters: num_epochs and early_stopping_patiences\n",
    "Epochs + patience control how long the model learns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc889196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune Basic Hyperparameters with TensorFlow: num_epochs and early_stopping_patiences\n",
    "\n",
    "# Define hyperparameters to tune\n",
    "early_stopping_patiences = [3, 5, 8]        # Number of epochs with no improvement after which training will be stopped\n",
    "num_epochs = [50, 80, 120]                  # Number of complete passes through the training dataset to update model weights\n",
    "\n",
    "# Previous best results from learning rate and batch size tuning for regression and classification\n",
    "best_learning_rate_reg = 0.0005\n",
    "best_batch_size_reg = 32\n",
    "best_learning_rate_cls = 0.0005\n",
    "best_batch_size_cls = 32\n",
    "\n",
    "dropout_rates = None                        # No dropout for this tuning phase as it will be tuned later\n",
    "\n",
    "# Track best results\n",
    "# --- REGRESSION MODEL ---\n",
    "best_val_loss_reg = float('inf')            # Initialize best validation loss to infinity(for minimization, as lower is better)\n",
    "best_hyperparams_reg = {}                   # Dictionary to store best hyperparameters for regression   \n",
    "\n",
    "# --- CLASSIFICATION MODEL ---\n",
    "best_val_loss_cls = float('inf')            \n",
    "best_hyperparams_cls = {}                   \n",
    "\n",
    "# Combined Tuning Loop for REGRESSION MOODEL\n",
    "for patience in early_stopping_patiences:\n",
    "    for epochs in num_epochs:\n",
    "        print(f'Tuning with Early Stopping Patience: {patience}, Number of Epochs: {epochs}')\n",
    "        \n",
    "        # Clear previous models and free up memory to avoid potential out-of-memory errors during training\n",
    "        clear_gpu_memory()\n",
    "\n",
    "        # Create (using the function) tf.data datasets (training + validation)\n",
    "        train_dataset_reg, val_dataset_reg = create_datasets_tuning_reg(X_train_norm, y_train_reg, X_val_norm, y_val_reg, best_batch_size_reg)  \n",
    "        \n",
    "        # Build and compile regression model\n",
    "        model_reg = build_base_cnn_reg(dropout_rate=None)           # Build regression model without dropout as it will be tuned later\n",
    "\n",
    "        model_reg.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=best_learning_rate_reg),\n",
    "                          loss='mean_squared_error',\n",
    "                          metrics=['mean_absolute_error'])\n",
    "        \n",
    "        # Define Early Stopping callback\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
    "        \n",
    "        # Train regression model\n",
    "        history_reg = model_reg.fit(train_dataset_reg,\n",
    "                                    validation_data=val_dataset_reg,\n",
    "                                    epochs=epochs,\n",
    "                                    callbacks=[early_stopping],\n",
    "                                    verbose=2)\n",
    "        \n",
    "        # Evaluate regression model on validation set\n",
    "        val_loss_reg = min(history_reg.history['val_loss'])                  # Get the minimum validation loss from the list of validation losses for all epochs of that run\n",
    "        print(f'Regression Validation Loss: {val_loss_reg}')\n",
    "        \n",
    "        # Update best regression hyperparameters if improved at end of every run (tuning iteration)\n",
    "        if val_loss_reg < best_val_loss_reg:\n",
    "            best_val_loss_reg = val_loss_reg\n",
    "            best_hyperparams_reg = {'early_stopping_patience': patience, 'num_epochs': epochs}\n",
    "        \n",
    "        # Clear previous models and free up memory to avoid potential out-of-memory errors during training\n",
    "        clear_gpu_memory()\n",
    "\n",
    "# Combined Tuning Loop for CLASSIFICATION MODEL\n",
    "for patience in early_stopping_patiences:\n",
    "    for epochs in num_epochs:\n",
    "        print(f'Tuning with Early Stopping Patience: {patience}, Number of Epochs: {epochs}')\n",
    "\n",
    "        # Each time before building a new model, clear previous models and free up memory to avoid potential out-of-memory errors during training\n",
    "        clear_gpu_memory()\n",
    "\n",
    "        # Create (using the function) tf.data datasets (training + validation)\n",
    "        train_dataset_cls, val_dataset_cls = create_datasets_tuning_cls(X_train_norm, y_train_cls_ohe, X_val_norm, y_val_cls_ohe, best_batch_size_cls)  \n",
    "\n",
    "        # Build and compile classification model\n",
    "        model_cls = build_base_cnn_cls(dropout_rate=None)          # Build classification model without dropout as it will be tuned later\n",
    "\n",
    "        model_cls.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=best_learning_rate_cls),\n",
    "                            loss='categorical_crossentropy',\n",
    "                            metrics=['accuracy'])\n",
    "        \n",
    "        # Define Early Stopping callback\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
    "        \n",
    "        # Train classification model\n",
    "        history_cls = model_cls.fit(train_dataset_cls,\n",
    "                                    validation_data=val_dataset_cls,\n",
    "                                    epochs=epochs,\n",
    "                                    callbacks=[early_stopping],\n",
    "                                    verbose=2)\n",
    "        \n",
    "        # Evaluate classification model on validation set \n",
    "        val_loss_cls = min(history_cls.history['val_loss'])                  # Get the minimum validation loss from the list of validation losses for all epochs of that run\n",
    "        print(f'Classification Validation Loss: {val_loss_cls}')\n",
    "        # Update best classification hyperparameters if improved at end of every run (tuning iteration)\n",
    "        if val_loss_cls < best_val_loss_cls:\n",
    "            best_val_loss_cls = val_loss_cls\n",
    "            best_hyperparams_cls = {'early_stopping_patience': patience, 'num_epochs': epochs}\n",
    "\n",
    "\n",
    "        # Clear previous models and free up memory to avoid potential out-of-memory errors during training\n",
    "        clear_gpu_memory()\n",
    "\n",
    "# Display best hyperparameters found\n",
    "print(\"Best Hyperparameters for Regression:\")\n",
    "print(best_hyperparams_reg)\n",
    "print(f\"Best Validation Loss for Regression: {best_val_loss_reg}\")\n",
    "\n",
    "print(\"\\nBest Hyperparameters for Classification:\")\n",
    "print(best_hyperparams_cls)\n",
    "print(f\"Best Validation Loss for Classification: {best_val_loss_cls}\")\n",
    "\n",
    "# Expected number of training runs: 3 (patience) * 3 (epochs) = 9 runs per model = 18 total\n",
    "# Expected time per epoch with CPU: ~20-40 seconds\n",
    "# Expected time per epoch with GPU: ~4-10 seconds\n",
    "# Total expected time with CPU : 18*20*20 seconds = ~2-4 hours (with early stopping, actual time will eventually be lower, even more than half)\n",
    "# Tota l expected time with GPU : 18*20*4 seconds = ~ 24 minutes - 48 minutes (with early stopping, actual time will eventually be lower, even more than half)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d0dc1d",
   "metadata": {},
   "source": [
    "Best Hyperparameters for Regression: {'early_stopping_patience': 3, 'num_epochs': 50}\n",
    "\n",
    "Best Validation Loss for Regression: 764.4470825195312\n",
    "\n",
    "Best Hyperparameters for Classification: {'early_stopping_patience': 8, 'num_epochs': 120}\n",
    "\n",
    "Best Validation Loss for Classification: 1.1832056045532227"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0e9fca",
   "metadata": {},
   "source": [
    "#### 1.6.3 Tune Basic Hyperparameters: Optimizer + Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05cffa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune Basic Hyperparameters with TensorFlow: dropout rate with Adam optimizer\n",
    "\n",
    "# Define hyperparameters to tune\n",
    "optimizers = ['adam']                       # Only 'adam' optimizer to reduce computational load\n",
    "dropout_rates = [0.3, 0.5]                  # Different dropout rates to try\n",
    "\n",
    "# the alternative optimizer 'sgd' is not used here to reduce computational load (also sgd requires more careful learning rate tuning)\n",
    "# sgd would require \n",
    "\n",
    "# Previous best results from learning rate and batch size tuning for regression and classification\n",
    "best_learning_rate_reg = 0.0005\n",
    "best_batch_size_reg = 32\n",
    "best_learning_rate_cls = 0.0005\n",
    "best_batch_size_cls = 32\n",
    "best_early_stopping_patience_reg = 3\n",
    "best_early_stopping_patience_cls = 8\n",
    "best_num_epochs_reg = 50\n",
    "best_num_epochs_cls = 120\n",
    "\n",
    "# Track best results\n",
    "# --- REGRESSION MODEL ---\n",
    "best_val_loss_reg = float('inf')            # Initialize best validation loss to infinity(for minimization, as lower is better)\n",
    "best_hyperparams_reg = {}                   # Dictionary to store best hyperparameters for regression\n",
    "# --- CLASSIFICATION MODEL ---\n",
    "best_val_loss_cls = float('inf')            # Initialize best validation loss to infinity(for minimization, as lower is better)\n",
    "best_hyperparams_cls = {}                   # Dictionary to store best hyperparameters for classification\n",
    "\n",
    "# Combined Tuning Loop for REGRESSION MODEL\n",
    "for opt in optimizers:\n",
    "    for dropout_rate in dropout_rates:\n",
    "        print(f'Tuning with Optimizer: {opt}, Dropout Rate: {dropout_rate}')\n",
    "        \n",
    "        # Clear previous models and free up memory to avoid potential out-of-memory errors during training\n",
    "        clear_gpu_memory()\n",
    "        \n",
    "        # Create tf.data datasets (training + validation) using the function to avoid OOM (out-of-memory) errors\n",
    "        train_dataset_reg, val_dataset_reg = create_datasets_tuning_reg(X_train_norm, y_train_reg, X_val_norm, y_val_reg, best_batch_size_reg)\n",
    "\n",
    "        # Build (using the function) and compile regression model\n",
    "        model_reg = build_base_cnn_reg(dropout_rate=dropout_rate)\n",
    "        \n",
    "        # Choose optimizer\n",
    "        if opt == 'adam':\n",
    "            optimizer_reg = tf.keras.optimizers.Adam(learning_rate=best_learning_rate_reg)\n",
    "        elif opt == 'sgd':\n",
    "            optimizer_reg = tf.keras.optimizers.SGD(learning_rate=best_learning_rate_reg)\n",
    "        \n",
    "        model_reg.compile(optimizer=optimizer_reg,\n",
    "                          loss='mean_squared_error',\n",
    "                          metrics=['mean_absolute_error'])\n",
    "        \n",
    "        # Define Early Stopping callback\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(          # it stops training when the validation loss stops improving after a certain number of epochs defined by patience\n",
    "            monitor='val_loss',\n",
    "            patience=best_early_stopping_patience_reg,\n",
    "            restore_best_weights=True)\n",
    "        \n",
    "        # Train regression model\n",
    "        history_reg = model_reg.fit(train_dataset_reg,\n",
    "                                    validation_data=val_dataset_reg,\n",
    "                                    epochs=best_num_epochs_reg,\n",
    "                                    verbose=2)\n",
    "        \n",
    "        # Evaluate regression model on validation set\n",
    "        val_loss_reg = min(history_reg.history['val_loss'])                  # Get the minimum validation loss from the list of validation losses for all epochs of that run\n",
    "        print(f'Regression Validation Loss: {val_loss_reg}')\n",
    "        \n",
    "        # Update best regression hyperparameters if improved at end of every run (tuning iteration)\n",
    "        if val_loss_reg < best_val_loss_reg:\n",
    "            best_val_loss_reg = val_loss_reg\n",
    "            best_hyperparams_reg = {\n",
    "                'optimizer': opt,\n",
    "                'dropout_rate': dropout_rate,\n",
    "            }\n",
    "        # Clear previous models and free up memory\n",
    "        clear_gpu_memory()\n",
    "\n",
    "# Combined Tuning Loop for CLASSIFICATION MODEL\n",
    "for opt in optimizers:\n",
    "    for dropout_rate in dropout_rates:\n",
    "        print(f'Tuning with Optimizer: {opt}, Dropout Rate: {dropout_rate}')\n",
    "\n",
    "        # Each time before building a new model, clear previous models and free up memory to avoid potential out-of-memory errors during training\n",
    "        clear_gpu_memory()\n",
    "\n",
    "        # Create tf.data datasets (training + validation) using the function to avoid OOM (out-of-memory) errors\n",
    "        train_dataset_cls, val_dataset_cls = create_datasets_tuning_cls(X_train_norm, y_train_cls_ohe, X_val_norm, y_val_cls_ohe, best_batch_size_cls)\n",
    "\n",
    "        # Build (using the function) and compile classification model   \n",
    "        model_cls = build_base_cnn_cls(dropout_rate=dropout_rate)\n",
    "\n",
    "        # Choose optimizer\n",
    "        if opt == 'adam':\n",
    "            optimizer_cls = tf.keras.optimizers.Adam(learning_rate=best_learning_rate_cls)\n",
    "        elif opt == 'sgd':\n",
    "            optimizer_cls = tf.keras.optimizers.SGD(learning_rate=best_learning_rate_cls)\n",
    "        \n",
    "        # Compile classification model\n",
    "        model_cls.compile(optimizer=optimizer_cls,\n",
    "                          loss='categorical_crossentropy',\n",
    "                          metrics=['accuracy'])\n",
    "        \n",
    "        # Define Early Stopping callback\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=best_early_stopping_patience_cls,\n",
    "            restore_best_weights=True)\n",
    "        \n",
    "        # Train classification model\n",
    "        history_cls = model_cls.fit(train_dataset_cls,\n",
    "                                    validation_data=val_dataset_cls,\n",
    "                                    epochs=best_num_epochs_cls,\n",
    "                                    verbose=2)\n",
    "        \n",
    "        # Evaluate classification model on validation set\n",
    "        val_loss_cls = min(history_cls.history['val_loss'])                  # Get the minimum validation loss from the list of validation losses for all epochs of that run\n",
    "        print(f'Classification Validation Loss: {val_loss_cls}')\n",
    "        \n",
    "        # Update best classification hyperparameters if improved at end of every run (tuning iteration)\n",
    "        if val_loss_cls < best_val_loss_cls:\n",
    "            best_val_loss_cls = val_loss_cls\n",
    "            best_hyperparams_cls = {\n",
    "                'optimizer': opt,\n",
    "                'dropout_rate': dropout_rate,\n",
    "            }\n",
    "        \n",
    "        # Clear previous models and free up memory to avoid potential out-of-memory errors during training\n",
    "        clear_gpu_memory()\n",
    "\n",
    "# Display best hyperparameters found\n",
    "print(\"Best Hyperparameters for Regression:\")\n",
    "print(best_hyperparams_reg)\n",
    "print(f\"Best Validation Loss for Regression: {best_val_loss_reg}\")\n",
    "\n",
    "print(\"Best Hyperparameters for Classification:\")\n",
    "print(best_hyperparams_cls)\n",
    "print(f\"Best Validation Loss for Classification: {best_val_loss_cls}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7301d807",
   "metadata": {},
   "source": [
    "Best Hyperparameters for Regression: {'optimizer': 'adam', 'dropout_rate': 0.3}\n",
    "\n",
    "Best Validation Loss for Regression: 603.141845703125\n",
    "\n",
    "Best Hyperparameters for Classification: {'optimizer': 'adam', 'dropout_rate': 0.5}\n",
    "\n",
    "Best Validation Loss for Classification: 1.122312068939209"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650f56ec",
   "metadata": {},
   "source": [
    "#### 1.7 Final CNN Baseline Models with best hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ce5dbc",
   "metadata": {},
   "source": [
    "##### 1.7.1. Improve the final CNN builders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abed7188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improve Final CNN Builders (Regression + Classification)\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization\n",
    "# Batach Normalization normalizes the activations of the previous layer at each batch,)\n",
    "# i.e., applies a transformation that maintains the mean activation close to 0 and the activation standard deviation close to 1.\n",
    "# This helps stabilize and speed up the training process by reducing internal covariate shift.\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout\n",
    "# Dropout is a regularization technique that randomly sets a fraction of input units to 0 at each update during training time,\n",
    "# which helps prevent overfitting by reducing reliance on specific neurons.\n",
    "\n",
    "# --- REGRESSION MODEL ---\n",
    "def final_build_base_cnn_reg(input_shape=(128, 128, 3), dropout_rate=None):\n",
    "    \"\"\"\n",
    "    Improved CNN for final regression model.\n",
    "    Adds Batch Normalization and slightly deeper feature extraction while \n",
    "    keeping the same interface and variable names.\n",
    "    \"\"\"\n",
    "\n",
    "    layers = [\n",
    "\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
    "        tf.keras.layers.BatchNormalization(),               # BatchNorm after Conv layer to stabilize activations\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "        # --- Block 2 ---\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        tf.keras.layers.BatchNormalization(),               # BatchNorm after Conv layer to stabilize activations\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "        tf.keras.layers.Flatten(),                          # Flatten 2D feature maps to 1D feature vector\n",
    "\n",
    "        # Dense layer\n",
    "        tf.keras.layers.Dense(128, activation='relu'),      # Fully connected layer with 128 neurons    \n",
    "        tf.keras.layers.BatchNormalization(),               # BatchNorm after Dense layer to stabilize activations\n",
    "        tf.keras.layers.Dropout(dropout_rate),              # Dropout for regularization (prevent overfitting)\n",
    "        \n",
    "        # Output layer\n",
    "        tf.keras.layers.Dense(1)                            # Regression output\n",
    "    ]\n",
    "    \n",
    "    return tf.keras.Sequential(layers)\n",
    "\n",
    "# --- CLASSIFICATION MODEL ---\n",
    "def final_build_base_cnn_cls(input_shape=(128, 128, 3), dropout_rate=None, num_classes=y_test_cls_ohe.shape[1]):\n",
    "    \"\"\"\n",
    "    Improved CNN for final classification model.\n",
    "    Adds Batch Normalization and slightly deeper feature extraction while \n",
    "    keeping the same interface and variable names.\n",
    "    \"\"\"\n",
    "\n",
    "    layers = [\n",
    "\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same',\n",
    "                               input_shape=input_shape),\n",
    "        tf.keras.layers.BatchNormalization(),               # BatchNorm after Conv layer to stabilize activations\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "        # --- Block 2 ---\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        tf.keras.layers.BatchNormalization(),               # BatchNorm after Conv layer to stabilize activations\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "        tf.keras.layers.Flatten(),                          # Flatten 2D feature maps to 1D feature vector\n",
    "\n",
    "        # Dense layer\n",
    "        tf.keras.layers.Dense(128, activation='relu'),      # Fully connected layer with 128 neurons    \n",
    "        tf.keras.layers.BatchNormalization(),               # BatchNorm after Dense layer to stabilize activations\n",
    "        tf.keras.layers.Dropout(dropout_rate),              # Dropout for regularization (prevent overfitting)\n",
    "        \n",
    "        # Output layer\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax')    # Classification output\n",
    "    ]\n",
    "    \n",
    "    return tf.keras.Sequential(layers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34470696",
   "metadata": {},
   "source": [
    "##### 1.7.2 Final Model Training and Evaluation with Best Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4728e0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final CNN Baseline Models for Regression and Classification with the best hyperparameters found\n",
    "\n",
    "# Previous best results from learning rate and batch size tuning for regression and classification\n",
    "best_learning_rate_reg = 0.0005\n",
    "best_batch_size_reg = 32\n",
    "best_learning_rate_cls = 0.0005\n",
    "best_batch_size_cls = 32\n",
    "best_early_stopping_patience_reg = 8\n",
    "best_early_stopping_patience_cls = 8\n",
    "best_num_epochs_reg = 80\n",
    "best_num_epochs_cls = 80\n",
    "optimizer_reg = 'adam'       \n",
    "optimizer_cls = 'adam'\n",
    "best_dropout_rate_reg = 0.3\n",
    "best_dropout_rate_cls = 0.5\n",
    "\n",
    "# --- REGRESSION MODEL ---\n",
    "\n",
    "# Clear previous models to free GPU memory\n",
    "clear_gpu_memory()\n",
    "\n",
    "# Create tf.data datasets (training + validation + test) to reduce memory usage with the function\n",
    "train_dataset_reg, val_dataset_reg, test_dataset_reg = create_datasets_final_reg(X_train_norm, y_train_reg, X_val_norm, y_val_reg, X_test_norm, y_test_reg, best_batch_size_reg)\n",
    "\n",
    "# Build (with the function) and compile regression model\n",
    "model_reg = final_build_base_cnn_reg(dropout_rate=best_dropout_rate_reg)\n",
    "\n",
    "# Choose optimizer\n",
    "if optimizer_reg == 'adam':                                                     # optimier is used to update model weights based on loss gradient during training\n",
    "    optimizer_reg_final  = tf.keras.optimizers.Adam(learning_rate=best_learning_rate_reg)  # Adam optimizer adapts learning rate for each parameter during training\n",
    "else:\n",
    "    optimizer_reg_final = tf.keras.optimizers.SGD(learning_rate=best_learning_rate_reg)   # Standard SGD uses a fixed learning rate to learn from data\n",
    "\n",
    "# Compile regression model\n",
    "model_reg.compile(optimizer=optimizer_reg_final,                          # configuring the model for training, specifying the optimizer, loss function, and evaluation metrics\n",
    "                      loss='mean_squared_error',\n",
    "                      metrics=['mean_absolute_error'])\n",
    "\n",
    "# Define Early Stopping callback\n",
    "early_stopping_reg = tf.keras.callbacks.EarlyStopping(monitor='val_loss',                       # stop training if validation loss does not improve for specified number of epochs\n",
    "                                                      patience=best_early_stopping_patience_reg, \n",
    "                                                      restore_best_weights=True) \n",
    "\n",
    "# Train regression model\n",
    "history_reg = model_reg.fit(train_dataset_reg,                      # .fit method trains the model for a fixed number of epochs (iterations on a dataset)\n",
    "                            epochs=best_num_epochs_reg, \n",
    "                            validation_data=val_dataset_reg, \n",
    "                            callbacks=[early_stopping_reg])         # early stopping of the epochs if validation loss does not improve, to prevent overfitting\n",
    "\n",
    "# Evaluate regression model on test set\n",
    "test_loss_reg, test_mae_reg = model_reg.evaluate(test_dataset_reg)          # .evaluate method computes the loss and metrics on the test dataset\n",
    "\n",
    "# Clear previous models and free up memory\n",
    "clear_gpu_memory()\n",
    "\n",
    "\n",
    "# --- CLASSIFICATION MODEL ---\n",
    "\n",
    "# Clear previous models to free GPU memory\n",
    "clear_gpu_memory()\n",
    "\n",
    "# Create tf.data datasets (for training + validation, test) to reduce memory usage with the function\n",
    "train_dataset_cls, val_dataset_cls, test_dataset_cls = create_datasets_final_cls(X_train_norm, y_train_cls_ohe, X_val_norm, y_val_cls_ohe, X_test_norm, y_test_cls_ohe, best_batch_size_cls)\n",
    "\n",
    "# Build (using the function) and compile classification model\n",
    "model_cls = final_build_base_cnn_cls(dropout_rate=best_dropout_rate_cls)\n",
    "\n",
    "# Choose optimizer\n",
    "if optimizer_cls == 'adam':\n",
    "    optimizer_cls_final = tf.keras.optimizers.Adam(learning_rate=best_learning_rate_cls)\n",
    "else:\n",
    "    optimizer_cls_final = tf.keras.optimizers.SGD(learning_rate=best_learning_rate_cls)\n",
    "\n",
    "# Compile classification model\n",
    "model_cls.compile(optimizer=optimizer_cls_final,  \n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "# Define Early Stopping callback\n",
    "early_stopping_cls = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=best_early_stopping_patience_cls, restore_best_weights=True)\n",
    "\n",
    "# Train classification model\n",
    "history_cls = model_cls.fit(train_dataset_cls, epochs=best_num_epochs_cls, validation_data=val_dataset_cls, callbacks=[early_stopping_cls])\n",
    "\n",
    "# Evaluate classification model on test set\n",
    "test_loss_cls, test_accuracy_cls = model_cls.evaluate(test_dataset_cls)\n",
    "\n",
    "# Clear previous models and free up memory\n",
    "clear_gpu_memory()\n",
    "\n",
    "# Display final test results\n",
    "print(\"Final Test Results for Regression Model:\")\n",
    "print(f\"Test Loss (MSE): {test_loss_reg:.4f}\")\n",
    "print(f\"Test MAE: {test_mae_reg:.4f}\")\n",
    "\n",
    "print(\"Final Test Results for Classification Model:\")\n",
    "print(f\"Test Loss: {test_loss_cls:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy_cls:.4f}\")\n",
    "\n",
    "print (\"Final Regression Model Summary:\")\n",
    "model_reg.summary()\n",
    "print (\"Final Classification Model Summary:\")\n",
    "model_cls.summary()\n",
    "\n",
    "# Save the final models\n",
    "model_reg.save('final_base_cnn_regression_model.keras')\n",
    "model_cls.save('final_base_cnn_classification_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce07f49",
   "metadata": {},
   "source": [
    "\n",
    "Final Test Results for Regression Model:\n",
    "- Test Loss (MSE): 394.7842\n",
    "- Test MAE: 15.8970\n",
    "\n",
    "Final Test Results for Classification Model:\n",
    "- Test Loss: 1.4021\n",
    "- Test Accuracy: 0.5595\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adl_tf_gpu_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
